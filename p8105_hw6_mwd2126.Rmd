---
title: "p8105_hw6_mwd2126.Rmd"
author: "Michael Denham"
date: "2022-12-03"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(modelr)
library(rnoaa)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


### Problem 1

To perform a bootstrap for linear regression with `tmax` as the response variable and `tmin` as the predictor variable, we can use the following code. First, we download the weather data.

```{r load data}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

Then, we set the number of bootstrap samples and make empty vectors to store the bootstrap estimates.

```{r set bootstrap}
n_bootstrap = 5000
r2_bootstrap = numeric(n_bootstrap)
log_beta0beta1_bootstrap = numeric(n_bootstrap)
```

Then we start bootstrap sampling.

```{r start bootstrap}
for (i in 1:n_bootstrap){

  #Sample with replacement from the original data
  bootstrap_sample = sample(1:nrow(weather_df), replace = TRUE)
  
  #Fit a linear regression model on the bootstrap sample
  bootstrap_model = lm(tmax ~ tmin, data = weather_df[bootstrap_sample, ])
  
  #Extract r2 and log(beta0 * beta1) from the bootstrap model
  r2_bootstrap[i] = broom::glance(bootstrap_model)$r.squared
  log_beta0beta1_bootstrap[i] = log(coef(bootstrap_model)[1] * coef(bootstrap_model)[2])
  
}
```

Then we plot the distribution of our bootstrap estimates.

```{r plot bootstrap}
#For r2_bootstrap
r2_bootstrap %>%
  data.frame() %>% 
  ggplot(aes(x = r2_bootstrap)) +
  geom_density() +
  labs(
    title = "Distribution of R-squared Estimates"
  )

#For log_beta0beta1_bootstrap
log_beta0beta1_bootstrap %>%
  data.frame() %>% 
  ggplot(aes(x = log_beta0beta1_bootstrap)) +
  geom_density() +
  labs(
    title = "Distribution of log(beta0 * beta1) Estimates"
  )
```

The resulting plots show the distribution of r2 and log(beta0 * beta1) estimates obtained from the bootstrap samples. They show that the value of `r2_bootstrap` is generally around 0.91 and the value of `log_beta0beta1_bootstrap` is generally around 2.01.

Now, we calculate 95% confidence intervals for r2 and log(beta0 * beta1). The 95% confidence intervals for these two quantities can be calculated by taking the 2.5% and 97.5% quantiles of the bootstrap estimates.

```{r}
r2_ci = quantile(r2_bootstrap, c(0.025, 0.975))
log_beta0beta1_ci = quantile(log_beta0beta1_bootstrap, c(0.025, 0.975))
```

In this case, the 95% confidence interval for `r2_bootstrap` is [`r round(r2_ci[1], 3)`, `r round(r2_ci[2], 3)`] and the 95% confidence interval for `log_beta0beta1_bootstrap` is [`r round(log_beta0beta1_ci[1], 3)`, `r round(log_beta0beta1_ci[2], 3)`].

### Problem 2

First, we'll start by importing the data from the Washington Post's Github page.

```{r read data}
homicide <- read.csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")
```

We can then create the `city_state` variable by combining the city and state columns, and create the binary variable `solved` indicating whether the homicide is solved by using the `ifelse()` function to assign 1 to homicides that are solved and 0 to those that are not.

```{r city_state and solved variables}
homicide = homicide %>% 
  mutate(
    city_state = str_c(city, ", ", state),
    resolved = ifelse(homicide$disposition == "Closed by arrest", 1, 0)
  )
```

Then we'll remove Dallas, TX; Phoenix, AZ; and Kansas City, MO because they donâ€™t report victim race and omit Tulsa, AL because this is a data entry mistake. We'll also remove any rows where the victim wasn't white or black and make sure the victim's age is numeric.

```{r remove rows}
homicide = homicide %>% 
  filter(city_state != "Dallas, TX" && city_state != "Phoenix, AZ" && city_state != "Kansas City, MO") %>%
  filter(city_state != "Tulsa, AL") %>% 
  filter(victim_race == "White" | victim_race == "Black") %>% 
  filter(victim_age != "Unknown") %>% 
  filter(victim_sex != "Unknown")

homicide$victim_age = as.numeric(homicide$victim_age)
```

We'll make a data frame including only victims in Baltimore, selecting for the variables we'll need in our subsequent analysis. Then, we'll run a generalized linear regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors.


```{r regression for baltimore}
baltimore = homicide %>% 
  filter(city_state == "Baltimore, MD") %>% 
  select(resolved, victim_age, victim_sex, victim_race)

fit_logistic = 
  baltimore %>% 
  glm(resolved ~ victim_age + victim_sex + victim_race , data = ., family = binomial())

fit_logistic = fit_logistic %>%
  broom::tidy() %>%
  mutate(OR = exp(estimate)) %>%
  select(term, log_OR = estimate, OR, p.value) %>% 
  knitr::kable(digits = 3) %>% 
  print()
```

The adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed is 0.426, with a confidence interval of [`r 0.426 + 0.138`, `r 0.426 - 0.138`], suggesting that in Baltimore homicides with male victims are less likely to be solved compared to those with female victims.

Now, we'll repeat this analysis for all the cities involved. We'll make a function and then map that function to all cities.

```{r}
city_state_list = unique(homicide$city_state)

homicide_city <- function(location) {
  df = homicide %>% 
    filter(city_state == location) %>%
    select(resolved, victim_age, victim_sex, victim_race)
  return(df)
}

city_state_dfs = map(city_state_list, homicide_city)

nest_city_homicides =
  homicide %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = map(data, ~ glm(resolved ~ victim_age + victim_sex + victim_race, data = .x, family = binomial())),
    results = map(models, broom::tidy, conf.int = TRUE, conf.level = 0.95)) %>% 
  select(-data, -models) %>% 
  unnest(results)
```

Now we'll neaten that information in a table.

```{r}
nest_city_homicides %>% 
  select(city_state, term, estimate) %>% 
  mutate(term = fct_inorder(term)) %>% 
  pivot_wider(
    names_from = term, values_from = estimate) %>% 
  knitr::kable(digits = 3)
```

Now we'll narrow in on the ORs and CIs for the impact of the victim's sex specifically.

```{r}
sex_hom = nest_city_homicides %>% 
  select(city_state, term, estimate, conf.low, conf.high) %>% 
  mutate(term = fct_inorder(term)) %>% 
  pivot_wider(
    names_from = term, values_from = estimate) %>%
  drop_na(victim_sexMale) %>%
  select(city_state, victim_sexMale, conf.low, conf.high) 

sex_hom %>% 
  knitr::kable(digits = 3)
```

And we'll plot it.

```{r}
sex_hom %>%
  mutate(city_state = fct_reorder(city_state, victim_sexMale)) %>% 
  ggplot(aes(x = city_state, y = victim_sexMale)) + 
  geom_point() +
  theme(axis.text.x = element_text(angle = 80, hjust = 1))
```
The plot might suggest that bigger cities seem to have a lower odds ratio. Notably, the odds ratio NYC is much lower than all the other cities included in this analysis.

### Problem 3

Load the data first.

```{r read data for problem 3}
bw <- read.csv("https://p8105.com/data/birthweight.csv")
```

Now we'll clean it up a bit.

```{r}
bw = bw %>% 
  drop_na() %>% 
  mutate(babysex = factor(if_else(babysex == 1, "male", "female")),
         frace = factor(recode(frace, '1' = "White", '2' = "Black", '3' = "Asian", '4' = "Puerto Rican", '8' = "Other", '9' = "Unknown")),
         malform = factor(if_else(malform == 1, "present", "absent")),
         mrace = factor(recode(mrace,'1' = "White", '2' = "Black", '3' = "Asian", '4' = "Puerto Rican", '8' = "Other", '9' = "Unknown")))
```

I'd like to use gestational age as a possible predictor for birth weight, since weight changes across gestation and prematurity can lead to lower birth weights. We'll also include mother's age as a covariate, as that can impact fetal development.

```{r}
fit = lm(bwt ~ gaweeks + momage, data = bw)

fit %>% 
ggplot(aes(x = gaweeks, y = bwt)) + 
  geom_point()

```





